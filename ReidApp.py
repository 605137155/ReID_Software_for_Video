

# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'new2.ui'
#
# Created by: PyQt5 UI code generator 5.15.7
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.

import random

from PyQt5 import QtCore, QtGui, QtWidgets

import time
import sys
import os.path as osp
import os

from PyQt5.QtCore import pyqtSignal, QThread
from yolo_ReID import *
from PyQt5.QtGui import QImage, QPixmap, QDoubleValidator
from PyQt5.QtWidgets import QApplication, QWidget, QLineEdit, QLabel, QFormLayout



class VideoThread(QThread):
    new_frame_signal = pyqtSignal(QPixmap)
    # number = pyqtSignal(int)

    def __init__(self, MainWindow):
        super().__init__()
        self.MW = MainWindow
        self._is_running = True
        self._is_paused = False

    def run(self):
        print("å¼€å§‹ï¼ï¼ï¼")
        # yolov5çš„è¾“å…¥å¤§å°
        INPUT_WIDTH = 640
        INPUT_HEIGHT = 640
        time1 = time.time()
        colors = [(255, 255, 0), (0, 255, 0), (0, 255, 255), (255, 0, 0)]
        is_cuda = len(sys.argv) > 1 and sys.argv[1] == "cuda"

        # ç›®æ ‡æ£€æµ‹æ¨¡å‹å’Œreidæ¨¡å‹
        root_path = os.path.dirname(os.getcwd())  # é¡¹ç›®æ ¹ç›®å½•

        print(root_path)
        yolov5_path = osp.join(root_path, 'config_files', 'yolov5s.onnx')
        reid_path = osp.join(root_path, 'config_files', 'reid_resnet50.onnx')
        class_path = osp.join(root_path, 'config_files', 'classes.txt')

        print("è·¯å¾„")
        print(yolov5_path)
        net = build_model(is_cuda, yolov5_path)
        reidModule = reid_model(is_cuda, reid_path)
        class_list = load_classes(class_path)
        threshold_similarity = self.MW.threshold
        print("ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š")
        print(threshold_similarity)
        # åŠ è½½è§†é¢‘
        capture = load_capture(self.MW.video_path)
        start = time.time_ns()
        frame_count = 0
        total_frames = 0
        fps = -1

        # è§†é¢‘è§„æ ¼
        width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
        print(width)
        print(height)
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        fpss = capture.get(cv2.CAP_PROP_FPS)

        out = cv2.VideoWriter('output.mp4', fourcc, fpss, (width, height))

        # è·å–targetçš„feature
        target = cv2.imread(self.MW.target_path)
        target_feature = reidFeatrueExtract(target, reidModule)
        target_feature_norm = target_feature / np.linalg.norm(target_feature)
        print("å¼€å§‹å¸§å¤„ç†ï¼")
        while self._is_running:
                # å¼€å§‹å¸§å¤„ç†
                start_clock = time.time()
                if not self._is_paused:
                    # Frame å°†è·å¾—è§†é¢‘ä¸­/ç›¸æœºä¸­çš„ä¸‹ä¸€å¸§çš„(é€šè¿‡â€œcapâ€)
                    # Ret å°†ä»ç›¸æœºä¸­è·å–çš„å¸§ä¸­è·å¾—è¿”å›å€¼ï¼Œè¦ä¹ˆä¸ºtrueï¼Œè¦ä¹ˆä¸ºfalse
                    ret, frame = capture.read()
                    if frame is None:
                        print("End of stream")
                        break
                    # ä¸€åŠçš„æ¦‚ç‡ä¸åšæ£€æµ‹ï¼Œç›´æ¥ä¿å­˜
                    f_random = random.random()
                    if f_random > 1:
                        frame_count += 1
                        total_frames += 1
                        # ä¿å­˜
                        out.write(frame)
                        # cv2 BGR è½¬ PYQT5 RBG
                        height, width, bytesPerComponent = frame.shape
                        bytesPerLine = 3 * width
                        cv2.cvtColor(frame, cv2.COLOR_BGR2RGB, frame)
                        QImg = QImage(frame.data, width, height, bytesPerLine, QImage.Format_RGB888)
                        pixmap = QPixmap.fromImage(QImg)
                        self.new_frame_signal.emit(pixmap)
                        continue

                    # åŸå¸§çš„æ·±æ‹·è´
                    # f_old = copy.deepcopy(frame)
                    # ç›®æ ‡æ£€æµ‹(è¡Œäººæ£€æµ‹)
                    inputImage = format_yolov5(frame)
                    outs = detect(inputImage, net, INPUT_WIDTH, INPUT_HEIGHT)
                    class_ids, confidences, boxes = wrap_detection(inputImage, outs[0], INPUT_WIDTH, INPUT_HEIGHT)
                    frame_count += 1
                    total_frames += 1
                    features = []
                    coordinate = []
                    # ç»˜åˆ¶è¡Œäººæ¡†ï¼Œä¸”æå–æ‰€æœ‰è¡Œäººç‰¹å¾ï¼ŒåŠå…¶å¯¹åº”çš„å¸§åæ ‡box
                    for (classid, confidence, box) in zip(class_ids, confidences, boxes):
                        # å¦‚æœæ˜¯è¡Œäººpersonçš„æ£€æµ‹æ¡†ï¼Œåˆ™æå–è¯¥è¡Œäººçš„ç‰¹å¾
                        if classid == 0:
                            color = colors[int(classid) % len(colors)]
                            a = np.array(frame[box[1]:box[1] + box[3], box[0]:box[0] + box[2]])
                            if a.shape[0] <= 0 or a.shape[1] <= 0:  # éæ³•boxè¿‡æ»¤img
                                continue
                            f = reidFeatrueExtract(a, reidModule)
                            f_norm = f / np.linalg.norm(f)
                            cos_sim = np.dot(f_norm, target_feature_norm)
                            # cos_sim = 0
                            # f = 0
                            # å¯¹æ¯”è¯†åˆ«
                            if cos_sim > threshold_similarity:
                                print(cos_sim)
                                # è¾“å‡ºäººç‰©å‡ºç°æ—¶é—´
                                p_clock = time.time()
                                second = int(p_clock - start_clock)
                                minute = int(second / 60)
                                print("{}åˆ†{}ç§’".format(minute, second))
                                # æ ‡è®°ä»»åŠ¡æ¡†æ¡†
                                cv2.rectangle(frame, box, color, 2)
                                cv2.rectangle(frame, (box[0], box[1] - 20), (box[0] + box[2], box[1]), color, -1)
                                cv2.putText(frame, class_list[classid], (box[0], box[1] - 10),
                                            cv2.FONT_HERSHEY_SIMPLEX, .5,
                                            (0, 0, 0))
                            features.append(f)
                            # personå¯¹åº”åæ ‡
                            coordinate.append(box)
                        else:
                            continue

                    if frame_count >= 30:
                        end = time.time()
                        fps = 1000000000 * frame_count / (end - start)
                        frame_count = 0
                        start = time.time_ns()

                    if fps > 0:
                        fps_label = "FPS: %.2f" % fps
                        cv2.putText(frame, fps_label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

                    # ä¿å­˜
                    out.write(frame)
                    # cv2 BGR è½¬ PYQT5 RBG
                    height, width, bytesPerComponent = frame.shape
                    bytesPerLine = 3 * width
                    cv2.cvtColor(frame, cv2.COLOR_BGR2RGB, frame)
                    QImg = QImage(frame.data, width, height, bytesPerLine, QImage.Format_RGB888)
                    pixmap = QPixmap.fromImage(QImg)
                    #å‘é€å¤„ç†åçš„ç…§ç‰‡åˆ°MainWindowsçš„ä¸»çº¿ç¨‹
                    self.new_frame_signal.emit(pixmap)

                    # if cv2.waitKey(1) > -1:
                    #     print("finished by user1")

        print("Total frames: " + str(total_frames))
        time2 = time.time()
        print("total time: " + str(time2 - time1))
        capture.release()
        out.release()
        cv2.destroyAllWindows()


    def pause(self):
        print("æš‚åœ")
        self._is_paused = True


    def resume(self):
        print("æ¢å¤")
        self._is_paused = False

    def close(self):
        print("å…³é—­")
        self._is_running = False

class Ui_MainWindow(object):

    def __init__(self):
        super().__init__()

    def setupUi(self, MainWindow):
        MainWindow.setObjectName("MainWindow")
        MainWindow.title = "è¡Œäººé‡è¯†åˆ«åº”ç”¨"
        # è·å–æ˜¾ç¤ºå™¨åˆ†è¾¨ç‡
        MainWindow.desktop = QApplication.desktop()
        MainWindow.screenRect = MainWindow.desktop.screenGeometry()
        MainWindow.screenheight = MainWindow.screenRect.height()
        MainWindow.screenwidth = MainWindow.screenRect.width()
        MainWindow.height = int(MainWindow.screenheight * 0.7)
        MainWindow.width = int(MainWindow.screenwidth * 0.7)

        print("Screen height {}".format(MainWindow.screenheight))
        print("Screen width {}".format(MainWindow.screenwidth))

        MainWindow.resize(MainWindow.width, MainWindow.height)
        MainWindow.wid = QWidget(MainWindow)
        MainWindow.setCentralWidget(MainWindow.wid)
        MainWindow.setWindowTitle(MainWindow.title)
        # MainWindow.initUI()

        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName("centralwidget")
        # å¯¼å…¥å›¾ç‰‡ï¼ˆ->,ğŸ‘‡,é•¿ï¼Œå®½ï¼‰
        self.Image = QtWidgets.QPushButton(self.centralwidget)
        self.Image.setGeometry(QtCore.QRect(50, 70, 200, 100))
        self.Image.setObjectName("Image")
        self.Image.clicked.connect(self.From_files)
        # å¯¼å…¥è§†é¢‘
        self.Video = QtWidgets.QPushButton(self.centralwidget)
        self.Video.setGeometry(QtCore.QRect(50, 326, 200, 100))
        self.Video.setObjectName("Video")
        self.Video.clicked.connect(self.Load_Videl)

        # é€‰æ‹©æ¨¡å‹
        self.Model = QtWidgets.QPushButton(self.centralwidget)
        self.Model.setGeometry(QtCore.QRect(50, 436, 200, 100))
        self.Model.setObjectName("Model")
        self.Model.clicked.connect(self.Load_Model)

        # ReIDæ¨ç†
        self.Inference = QtWidgets.QPushButton(self.centralwidget)
        self.Inference.setGeometry(QtCore.QRect(50, 556, 80, 100))
        self.Inference.setObjectName("Inference")
        # self.Inference.clicked.connect(self.ReID_Inference)
        self.Inference.clicked.connect(self.ReID_Inference)

        # æš‚åœ
        self.Stop = QtWidgets.QPushButton(self.centralwidget)
        self.Stop.setGeometry(QtCore.QRect(170, 556, 80, 100))
        self.Stop.setObjectName("Stop")
        self.Stop.clicked.connect(self.Inference_Pause)

        # å”¤é†’
        self.Wake = QtWidgets.QPushButton(self.centralwidget)
        self.Wake.setGeometry(QtCore.QRect(170, 700, 80, 100))
        self.Wake.setObjectName("Wake")
        self.Wake.clicked.connect(self.Inference_Wake)

        # å…³é—­çº¿ç¨‹
        self.Close = QtWidgets.QPushButton(self.centralwidget)
        self.Close.setGeometry(QtCore.QRect(50, 700, 80, 100))
        self.Close.setObjectName("Close")
        self.Close.clicked.connect(self.Inference_Close)



        # æ˜¾ç¤ºçš„å›¾ç‰‡
        self.The_image = QtWidgets.QLabel(self.centralwidget)

        self.The_image.setGeometry(QtCore.QRect(300, 70, 168, 256))
        self.The_image.setObjectName("The_image")
        self.The_image.setScaledContents(True)
        self.The_image.setFrameShape(QtWidgets.QFrame.Box)

        # å¯¼å…¥çš„è§†é¢‘
        self.The_video = QtWidgets.QLabel(self.centralwidget)
        self.The_video.setGeometry(QtCore.QRect(560, 70, 168, 256))
        self.The_video.setObjectName("The_video")
        self.The_video.setFrameShape(QtWidgets.QFrame.Box)

        #æ˜¾ç¤ºé€‰æ‹©çš„æ¨¡å‹
        self.The_model = QtWidgets.QLabel(self.centralwidget)
        self.The_model.setGeometry(QtCore.QRect(820, 70, 168, 256))
        self.The_model.setObjectName("The_model")

        #å¡«å…¥çš„ç›¸ä¼¼åº¦é˜ˆå€¼
        flo = QFormLayout() #è¡¨å•å¸ƒå±€
        self.The_threshold = QLineEdit()
        self.The_threshold.textChanged.connect(self.updateNumber)
        self.The_threshold.setGeometry(QtCore.QRect(1080, 70, 168, 256))
        self.The_threshold.setObjectName("The_threshold")

        self.threshold_label = QLabel('0')
        self.setCentralWidget(self.The_threshold)
        self.statusBar().addPermanentWidget(self.threshold_label)
        self.threshold_label.setObjectName("threshold_label")

        #è®¾ç½®èŒƒå›´éªŒè¯å™¨
        self.threshold_validator = QDoubleValidator()
        self.threshold_validator.setRange(0.0,1.0)
        self.threshold_validator.setNotation(QDoubleValidator.StandardNotation)
        self.threshold_validator.setDecimals(5)
        self.The_threshold.setValidator(self.threshold_validator)


        flo.addRow('é˜ˆå€¼', self.The_threshold)

        # MainWindow.setLayout(flo)

        # æ¨ç†çš„è§†é¢‘å¸§
        self.The_inference = QtWidgets.QLabel(self.centralwidget)
        self.The_inference.setGeometry(QtCore.QRect(300, 360, 1280, 720))
        self.The_inference.setObjectName("The_inference")
        self.The_inference.setScaledContents(True) # è‡ªé€‚åº”å¤§å°
        self.The_inference.setFrameShape(QtWidgets.QFrame.Box)

        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(300, 600, 827, 22))
        self.menubar.setObjectName("menubar")
        MainWindow.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(MainWindow)
        self.statusbar.setObjectName("statusbar")
        MainWindow.setStatusBar(self.statusbar)

        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

        # çº¿ç¨‹æ§åˆ¶

    #é˜ˆå€¼æ›´æ–°
    def updateNumber(self, text):
        try:
            number = float(text)
            print("æ•°å­—æ”¹å˜ï¼")
            print(number)
            self.threshold = number
        except ValueError:
            pass

    def From_files(self):
        # è·å–å½“åœ°é€‰å–å›¾ç‰‡çš„ç»å¯¹urlï¼Œå¹¶å°†å…¶æ˜¾ç¤ºåœ¨The_imageçš„labelæ§ä»¶ä¸­
        the_image_url = QtWidgets.QFileDialog.getOpenFileName(None, 'select image', '', '')
        url = str(the_image_url).split("'")
        # ['(', 'C:/Users/Minghui_Zhang/Desktop/yolov5-opencv-cpp-python-main/target.jpg', ', ', 'All Files (*)', ')']
        print(url[1])
        # self.The_Url.setText(QtCore.QCoreApplication.translate("MainWindow", "è·¯å¾„ï¼š" + url[1]))

        self.target_path = url[1]
        # åˆ¤æ–­æ–‡ä»¶æ˜¯å¦æ˜¯å›¾ç‰‡
        print(self.target_path)
        try:
            if not is_image_file(self.target_path):
                raise ValueError("Not a picture file")
        except ValueError:
            # å¼¹å‡ºæç¤ºæ¡†
            QtWidgets.QMessageBox.critical(self, "Error", "è¯·é€‰æ‹©ä¸€ä¸ªåˆæ³•çš„å›¾ç‰‡æ–‡ä»¶!")
            self.The_image.setText("è¯·å†å¯¼å…¥å›¾ç‰‡")
            return
        img = QtGui.QPixmap(url[1])
        self.The_image.setPixmap(img)

    def Load_Videl(self):  # åŠ è½½è§†é¢‘çš„ä¿¡å·
        the_video_url = QtWidgets.QFileDialog.getOpenFileName(None, 'select video', '', '')
        video_url = str(the_video_url).split("'")
        # è°ƒç”¨yolo.pyçš„æ–¹æ³•åŠ è½½cv2æ ¼å¼çš„è§†é¢‘,æ”¾å…¥video_caputreå±æ€§
        self.video_path = video_url[1]

        # åˆ¤æ–­æ–‡ä»¶æ˜¯å¦æ˜¯è§†é¢‘
        try:
            if not is_video_file(self.video_path):
                raise ValueError("Not a video file")
        except ValueError:
            # å¼¹å‡ºæç¤ºæ¡†
            QtWidgets.QMessageBox.critical(self, "Error", "è¯·é€‰æ‹©ä¸€ä¸ªåˆæ³•çš„è§†é¢‘æ–‡ä»¶!")
            self.The_video.setText("è¯·å†å¯¼å…¥è§†é¢‘")
            return

        # ç»™å‡ºæç¤º
        # QtWidgets.QMessageBox.information(self, "Success", "The selected file is a valid video file.")
        print("å¯¼å…¥è·¯å¾„æˆåŠŸ")
        self.The_video.setText("å·²å¯¼å…¥è§†é¢‘")

    def Load_Model(self):
        self.model = ...
        self.threshold = 0.9998
        print(self.threshold)
        self.The_model.setText("å·²é€‰æ‹©é»˜è®¤æ¨¡å‹")


    def ReID_Inference(self):
        # self.inference = True

        # åˆ›å»º VideoThread å¯¹è±¡å¹¶å¯åŠ¨çº¿ç¨‹
        self.video_thread = VideoThread(self)
        self.video_thread.new_frame_signal.connect(self.detect_and_show_frame)
        self.video_thread.start()

    def detect_and_show_frame(self, new_frame_signal):
        # print("ä¼ åˆ°è¿™é‡Œæ¥äº†ï¼")
        self.The_inference.setPixmap(new_frame_signal)




    def Inference_Pause(self):
        self.video_thread.pause()

    def Inference_Wake(self):
        self.video_thread.resume()

    def Inference_Close(self):
        # å…³é—­ VideoThread çº¿ç¨‹
        if self.video_thread is not None:
            # self.video_thread.pause()
            self.video_thread.close()
        print("ç»“æŸæ¨ç†")
        # event.accept()


    # def ReID_Inference(self):  # æ¨ç†
    #     self.inference = True
    #     print("å¼€å§‹ï¼ï¼ï¼")
    #
    #     # yolov5çš„è¾“å…¥å¤§å°
    #     INPUT_WIDTH = 640
    #     INPUT_HEIGHT = 640
    #     time1 = time.time()
    #     colors = [(255, 255, 0), (0, 255, 0), (0, 255, 255), (255, 0, 0)]
    #     is_cuda = len(sys.argv) > 1 and sys.argv[1] == "cuda"
    #
    #     # ç›®æ ‡æ£€æµ‹æ¨¡å‹å’Œreidæ¨¡å‹
    #     root_path = os.path.dirname(os.getcwd())  # é¡¹ç›®æ ¹ç›®å½•
    #
    #     print(root_path)
    #     yolov5_path = osp.join(root_path, 'config_files', 'yolov5s.onnx')
    #     reid_path = osp.join(root_path, 'config_files', 'mgn_.onnx')
    #     class_path = osp.join(root_path, 'config_files', 'classes.txt')
    #
    #     print("è·¯å¾„")
    #     print(yolov5_path)
    #     net = build_model(is_cuda, yolov5_path)
    #     reidModule = reid_model(is_cuda, reid_path)
    #     class_list = load_classes(class_path)
    #     threshold_similarity = self.threshold
    #     print("ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š")
    #     print(threshold_similarity)
    #     # åŠ è½½è§†é¢‘
    #     capture = load_capture(self.video_path)
    #     start = time.time_ns()
    #     frame_count = 0
    #     total_frames = 0
    #     fps = -1
    #
    #     # è§†é¢‘è§„æ ¼
    #     width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
    #     height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
    #     print(width)
    #     print(height)
    #     fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    #     fpss = capture.get(cv2.CAP_PROP_FPS)
    #
    #     out = cv2.VideoWriter('output.mp4', fourcc, fpss, (width, height))
    #
    #     # è·å–targetçš„feature
    #     target = cv2.imread(self.target_path)
    #     target_feature = reidFeatrueExtract(target, reidModule)
    #     target_feature_norm = target_feature / np.linalg.norm(target_feature)
    #
    #     # å¼€å§‹å¸§å¤„ç†
    #     start_clock = time.time()
    #     print("å¼€å§‹å¸§å¤„ç†")
    #     while self.inference:
    #
    #         # Frame å°†è·å¾—è§†é¢‘ä¸­/ç›¸æœºä¸­çš„ä¸‹ä¸€å¸§çš„(é€šè¿‡â€œcapâ€)
    #         # Ret å°†ä»ç›¸æœºä¸­è·å–çš„å¸§ä¸­è·å¾—è¿”å›å€¼ï¼Œè¦ä¹ˆä¸ºtrueï¼Œè¦ä¹ˆä¸ºfalse
    #         _, frame = capture.read()
    #
    #         if frame is None:
    #             print("End of stream")
    #             break
    #
    #         #ä¸€åŠçš„æ¦‚ç‡ä¸åšæ£€æµ‹ï¼Œç›´æ¥ä¿å­˜
    #         f_random = random.random()
    #         if f_random > 0.5 :
    #             frame_count += 1
    #             total_frames += 1
    #             # ä¿å­˜
    #             out.write(frame)
    #             # cv2 BGR è½¬ PYQT5 RBG
    #             height, width, bytesPerComponent = frame.shape
    #             bytesPerLine = 3 * width
    #             cv2.cvtColor(frame, cv2.COLOR_BGR2RGB, frame)
    #             QImg = QImage(frame.data, width, height, bytesPerLine, QImage.Format_RGB888)
    #             pixmap = QPixmap.fromImage(QImg)
    #             self.The_inference.setPixmap(pixmap)
    #
    #             if cv2.waitKey(1) > -1:
    #                 print("finished by user1")
    #
    #             # if self.inference:
    #             #     while self.paused:
    #             #         ...
    #
    #             # print(self.inference)
    #             if not self.inference:
    #                 print("finished by user2")
    #                 break
    #             print("è·³è¿‡")
    #             continue
    #
    #
    #         # åŸå¸§çš„æ·±æ‹·è´
    #         # f_old = copy.deepcopy(frame)
    #         # ç›®æ ‡æ£€æµ‹(è¡Œäººæ£€æµ‹)
    #         inputImage = format_yolov5(frame)
    #         outs = detect(inputImage, net, INPUT_WIDTH, INPUT_HEIGHT)
    #         class_ids, confidences, boxes = wrap_detection(inputImage, outs[0], INPUT_WIDTH, INPUT_HEIGHT)
    #         frame_count += 1
    #         total_frames += 1
    #         features = []
    #         coordinate = []
    #         # ç»˜åˆ¶è¡Œäººæ¡†ï¼Œä¸”æå–æ‰€æœ‰è¡Œäººç‰¹å¾ï¼ŒåŠå…¶å¯¹åº”çš„å¸§åæ ‡box
    #         for (classid, confidence, box) in zip(class_ids, confidences, boxes):
    #             # å¦‚æœæ˜¯è¡Œäººpersonçš„æ£€æµ‹æ¡†ï¼Œåˆ™æå–è¯¥è¡Œäººçš„ç‰¹å¾
    #             if classid == 0:
    #                 color = colors[int(classid) % len(colors)]
    #                 a = np.array(frame[box[1]:box[1] + box[3], box[0]:box[0] + box[2]])
    #                 if a.shape[0] <= 0 or a.shape[1] <= 0:  # éæ³•boxè¿‡æ»¤img
    #                     continue
    #                 f = reidFeatrueExtract(a, reidModule)
    #                 f_norm = f / np.linalg.norm(f)
    #                 cos_sim = np.dot(f_norm, target_feature_norm)
    #                 # cos_sim = 0
    #                 # f = 0
    #                 # å¯¹æ¯”è¯†åˆ«
    #                 if cos_sim > threshold_similarity:
    #                     print(cos_sim)
    #                     # è¾“å‡ºäººç‰©å‡ºç°æ—¶é—´
    #                     p_clock = time.time()
    #                     second = int(p_clock - start_clock)
    #                     minute = int(second / 60)
    #                     print("{}åˆ†{}ç§’".format(minute, second))
    #                     # æ ‡è®°ä»»åŠ¡æ¡†æ¡†
    #                     cv2.rectangle(frame, box, color, 2)
    #                     cv2.rectangle(frame, (box[0], box[1] - 20), (box[0] + box[2], box[1]), color, -1)
    #                     cv2.putText(frame, class_list[classid], (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, .5,
    #                                 (0, 0, 0))
    #                 features.append(f)
    #                 # personå¯¹åº”åæ ‡
    #                 coordinate.append(box)
    #             else:
    #                 continue
    #
    #         if frame_count >= 30:
    #             end = time.time()
    #             fps = 1000000000 * frame_count / (end - start)
    #             frame_count = 0
    #             start = time.time_ns()
    #
    #         if fps > 0:
    #             fps_label = "FPS: %.2f" % fps
    #             cv2.putText(frame, fps_label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
    #
    #         # ä¿å­˜
    #         out.write(frame)
    #         # cv2 BGR è½¬ PYQT5 RBG
    #         height, width, bytesPerComponent = frame.shape
    #         bytesPerLine = 3 * width
    #         cv2.cvtColor(frame, cv2.COLOR_BGR2RGB, frame)
    #         QImg = QImage(frame.data, width, height, bytesPerLine, QImage.Format_RGB888)
    #         pixmap = QPixmap.fromImage(QImg)
    #         self.The_inference.setPixmap(pixmap)
    #
    #         if cv2.waitKey(1) > -1:
    #             print("finished by user1")
    #
    #         # if self.inference:
    #         #     while self.paused:
    #         #         ...
    #
    #         # print(self.inference)
    #         if not self.inference:
    #             print("finished by user2")
    #             break
    #
    #     print("Total frames: " + str(total_frames))
    #     time2 = time.time()
    #     print("total time: " + str(time2 - time1))
    #     capture.release()
    #     out.release()
    #     cv2.destroyAllWindows()



    def retranslateUi(self, MainWindow):
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setWindowTitle(_translate("MainWindow", "è¡Œäººé‡è¯†åˆ«ç›‘æµ‹ç³»ç»Ÿ"))
        self.Image.setText(_translate("MainWindow", "å¯¼å…¥å›¾ç‰‡"))
        self.Video.setText(_translate("MainWindow", "å¯¼å…¥è§†é¢‘"))
        self.Model.setText(_translate("MainWindow", "é€‰æ‹©æ¨¡å‹"))
        self.Inference.setText(_translate("MainWindow", "ReIDæ¨ç†"))
        self.The_image.setText(_translate("MainWindow", "è¯·å¯¼å…¥å›¾ç‰‡"))
        self.The_video.setText(_translate("MainWindow", "è¯·å¯¼å…¥è§†é¢‘"))
        self.The_model.setText(_translate("MainWindow", "è¯·é€‰æ‹©æ¨¡å‹"))
        self.The_threshold.setText(_translate("MainWindow", "è¯·è¾“å…¥ç›¸ä¼¼åº¦é˜ˆå€¼"))
        self.The_inference.setText(_translate("MainWindow", "è¯·æ¨ç†"))
        self.Stop.setText(_translate("MainWindow", "æš‚åœæ¨ç†"))
        self.Wake.setText(_translate("MainWindow", "ç»§ç»­æ¨ç†"))
        self.Close.setText(_translate("MainWindow", "å…³é—­æ¨ç†"))



if __name__ == '__main__':
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())