

# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'new2.ui'
#
# Created by: PyQt5 UI code generator 5.15.7
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.

import random

from PyQt5 import QtCore, QtGui, QtWidgets

import time
import sys
import os.path as osp
import os

from PyQt5.QtCore import pyqtSignal, QThread
from yolo_ReID import *
from PyQt5.QtGui import QImage, QPixmap, QDoubleValidator
from PyQt5.QtWidgets import QApplication, QWidget, QLineEdit, QLabel, QFormLayout



class VideoThread(QThread):
    new_frame_signal = pyqtSignal(QPixmap)
    # number = pyqtSignal(int)

    def __init__(self, MainWindow):
        super().__init__()
        self.MW = MainWindow
        self._is_running = True
        self._is_paused = False

    def run(self):
        print("开始！！！")
        # yolov5的输入大小
        INPUT_WIDTH = 640
        INPUT_HEIGHT = 640
        time1 = time.time()
        colors = [(255, 255, 0), (0, 255, 0), (0, 255, 255), (255, 0, 0)]
        is_cuda = len(sys.argv) > 1 and sys.argv[1] == "cuda"

        # 目标检测模型和reid模型
        # root_path = os.path.dirname(os.getcwd())  # 项目根目录
        root_path = os.getcwd()
        print(root_path)
        yolov5_path = osp.join(root_path, 'config_files', 'yolov5s.onnx')
        reid_path = osp.join(root_path, 'config_files', 'reid_resnet50.onnx')
        class_path = osp.join(root_path, 'config_files', 'classes.txt')

        print("路径")
        print(yolov5_path)
        net = build_model(is_cuda, yolov5_path)
        reidModule = reid_model(is_cuda, reid_path)
        class_list = load_classes(class_path)
        threshold_similarity = self.MW.threshold
        print("相似度阈值：")
        print(threshold_similarity)
        # 加载视频
        capture = load_capture(self.MW.video_path)
        start = time.time_ns()
        frame_count = 0
        total_frames = 0
        fps = -1

        # 视频规格
        width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
        print(width)
        print(height)
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        fpss = capture.get(cv2.CAP_PROP_FPS)

        out = cv2.VideoWriter('output.mp4', fourcc, fpss, (width, height))

        # 获取target的feature
        target = cv2.imread(self.MW.target_path)
        target_feature = reidFeatrueExtract(target, reidModule)
        target_feature_norm = target_feature / np.linalg.norm(target_feature)
        print("开始帧处理！")
        while self._is_running:
                # 开始帧处理
                start_clock = time.time()
                if not self._is_paused:
                    # Frame 将获得视频中/相机中的下一帧的(通过“cap”)
                    # Ret 将从相机中获取的帧中获得返回值，要么为true，要么为false
                    ret, frame = capture.read()
                    if frame is None:
                        print("End of stream")
                        break
                    # 一半的概率不做检测，直接保存
                    f_random = random.random()
                    if f_random > 1:
                        frame_count += 1
                        total_frames += 1
                        # 保存
                        out.write(frame)
                        # cv2 BGR 转 PYQT5 RBG
                        height, width, bytesPerComponent = frame.shape
                        bytesPerLine = 3 * width
                        cv2.cvtColor(frame, cv2.COLOR_BGR2RGB, frame)
                        QImg = QImage(frame.data, width, height, bytesPerLine, QImage.Format_RGB888)
                        pixmap = QPixmap.fromImage(QImg)
                        self.new_frame_signal.emit(pixmap)
                        continue

                    # 原帧的深拷贝
                    # f_old = copy.deepcopy(frame)
                    # 目标检测(行人检测)
                    inputImage = format_yolov5(frame)
                    outs = detect(inputImage, net, INPUT_WIDTH, INPUT_HEIGHT)
                    class_ids, confidences, boxes = wrap_detection(inputImage, outs[0], INPUT_WIDTH, INPUT_HEIGHT)
                    frame_count += 1
                    total_frames += 1
                    features = []
                    coordinate = []
                    # 绘制行人框，且提取所有行人特征，及其对应的帧坐标box
                    for (classid, confidence, box) in zip(class_ids, confidences, boxes):
                        # 如果是行人person的检测框，则提取该行人的特征
                        if classid == 0:
                            color = colors[int(classid) % len(colors)]
                            a = np.array(frame[box[1]:box[1] + box[3], box[0]:box[0] + box[2]])
                            if a.shape[0] <= 0 or a.shape[1] <= 0:  # 非法box过滤img
                                continue
                            f = reidFeatrueExtract(a, reidModule)
                            f_norm = f / np.linalg.norm(f)
                            cos_sim = np.dot(f_norm, target_feature_norm)
                            # cos_sim = 0
                            # f = 0
                            # 对比识别
                            if cos_sim > threshold_similarity:
                                print(cos_sim)
                                # 输出人物出现时间
                                p_clock = time.time()
                                second = int(p_clock - start_clock)
                                minute = int(second / 60)
                                print("{}分{}秒".format(minute, second))
                                # 标记任务框框
                                cv2.rectangle(frame, box, color, 2)
                                cv2.rectangle(frame, (box[0], box[1] - 20), (box[0] + box[2], box[1]), color, -1)
                                cv2.putText(frame, class_list[classid], (box[0], box[1] - 10),
                                            cv2.FONT_HERSHEY_SIMPLEX, .5,
                                            (0, 0, 0))
                            features.append(f)
                            # person对应坐标
                            coordinate.append(box)
                        else:
                            continue

                    if frame_count >= 30:
                        end = time.time()
                        fps = 1000000000 * frame_count / (end - start)
                        frame_count = 0
                        start = time.time_ns()

                    if fps > 0:
                        fps_label = "FPS: %.2f" % fps
                        cv2.putText(frame, fps_label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

                    # 保存
                    out.write(frame)
                    # cv2 BGR 转 PYQT5 RBG
                    height, width, bytesPerComponent = frame.shape
                    bytesPerLine = 3 * width
                    cv2.cvtColor(frame, cv2.COLOR_BGR2RGB, frame)
                    QImg = QImage(frame.data, width, height, bytesPerLine, QImage.Format_RGB888)
                    pixmap = QPixmap.fromImage(QImg)
                    #发送处理后的照片到MainWindows的主线程
                    self.new_frame_signal.emit(pixmap)

                    # if cv2.waitKey(1) > -1:
                    #     print("finished by user1")

        print("Total frames: " + str(total_frames))
        time2 = time.time()
        print("total time: " + str(time2 - time1))
        capture.release()
        out.release()
        cv2.destroyAllWindows()


    def pause(self):
        print("暂停")
        self._is_paused = True


    def resume(self):
        print("恢复")
        self._is_paused = False

    def close(self):
        print("关闭")
        self._is_running = False

class Ui_MainWindow(object):

    def __init__(self):
        super().__init__()

    def setupUi(self, MainWindow):
        MainWindow.setObjectName("MainWindow")
        MainWindow.title = "行人重识别应用"
        # 获取显示器分辨率
        MainWindow.desktop = QApplication.desktop()
        MainWindow.screenRect = MainWindow.desktop.screenGeometry()
        MainWindow.screenheight = MainWindow.screenRect.height()
        MainWindow.screenwidth = MainWindow.screenRect.width()
        MainWindow.height = int(MainWindow.screenheight * 0.7)
        MainWindow.width = int(MainWindow.screenwidth * 0.7)

        print("Screen height {}".format(MainWindow.screenheight))
        print("Screen width {}".format(MainWindow.screenwidth))

        MainWindow.resize(MainWindow.width, MainWindow.height)
        MainWindow.wid = QWidget(MainWindow)
        MainWindow.setCentralWidget(MainWindow.wid)
        MainWindow.setWindowTitle(MainWindow.title)
        # MainWindow.initUI()

        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName("centralwidget")
        # 导入图片（->,👇,长，宽）
        self.Image = QtWidgets.QPushButton(self.centralwidget)
        self.Image.setGeometry(QtCore.QRect(50, 70, 200, 100))
        self.Image.setObjectName("Image")
        self.Image.clicked.connect(self.From_files)
        # 导入视频
        self.Video = QtWidgets.QPushButton(self.centralwidget)
        self.Video.setGeometry(QtCore.QRect(50, 326, 200, 100))
        self.Video.setObjectName("Video")
        self.Video.clicked.connect(self.Load_Videl)

        # 选择模型
        self.Model = QtWidgets.QPushButton(self.centralwidget)
        self.Model.setGeometry(QtCore.QRect(50, 436, 200, 100))
        self.Model.setObjectName("Model")
        self.Model.clicked.connect(self.Load_Model)

        # ReID推理
        self.Inference = QtWidgets.QPushButton(self.centralwidget)
        self.Inference.setGeometry(QtCore.QRect(50, 556, 80, 100))
        self.Inference.setObjectName("Inference")
        # self.Inference.clicked.connect(self.ReID_Inference)
        self.Inference.clicked.connect(self.ReID_Inference)

        # 暂停
        self.Stop = QtWidgets.QPushButton(self.centralwidget)
        self.Stop.setGeometry(QtCore.QRect(170, 556, 80, 100))
        self.Stop.setObjectName("Stop")
        self.Stop.clicked.connect(self.Inference_Pause)

        # 唤醒
        self.Wake = QtWidgets.QPushButton(self.centralwidget)
        self.Wake.setGeometry(QtCore.QRect(170, 700, 80, 100))
        self.Wake.setObjectName("Wake")
        self.Wake.clicked.connect(self.Inference_Wake)

        # 关闭线程
        self.Close = QtWidgets.QPushButton(self.centralwidget)
        self.Close.setGeometry(QtCore.QRect(50, 700, 80, 100))
        self.Close.setObjectName("Close")
        self.Close.clicked.connect(self.Inference_Close)



        # 显示的图片
        self.The_image = QtWidgets.QLabel(self.centralwidget)

        self.The_image.setGeometry(QtCore.QRect(300, 70, 168, 256))
        self.The_image.setObjectName("The_image")
        self.The_image.setScaledContents(True)
        self.The_image.setFrameShape(QtWidgets.QFrame.Box)

        # 导入的视频
        self.The_video = QtWidgets.QLabel(self.centralwidget)
        self.The_video.setGeometry(QtCore.QRect(560, 70, 168, 256))
        self.The_video.setObjectName("The_video")
        self.The_video.setFrameShape(QtWidgets.QFrame.Box)

        #显示选择的模型
        self.The_model = QtWidgets.QLabel(self.centralwidget)
        self.The_model.setGeometry(QtCore.QRect(820, 70, 168, 256))
        self.The_model.setObjectName("The_model")

        #填入的相似度阈值
        flo = QFormLayout() #表单布局
        self.The_threshold = QLineEdit()
        self.The_threshold.textChanged.connect(self.updateNumber)
        self.The_threshold.setGeometry(QtCore.QRect(1080, 70, 168, 256))
        self.The_threshold.setObjectName("The_threshold")

        self.threshold_label = QLabel('0')
        self.setCentralWidget(self.The_threshold)
        self.statusBar().addPermanentWidget(self.threshold_label)
        self.threshold_label.setObjectName("threshold_label")

        #设置范围验证器
        self.threshold_validator = QDoubleValidator()
        self.threshold_validator.setRange(0.0,1.0)
        self.threshold_validator.setNotation(QDoubleValidator.StandardNotation)
        self.threshold_validator.setDecimals(5)
        self.The_threshold.setValidator(self.threshold_validator)


        flo.addRow('阈值', self.The_threshold)

        # MainWindow.setLayout(flo)

        # 推理的视频帧
        self.The_inference = QtWidgets.QLabel(self.centralwidget)
        self.The_inference.setGeometry(QtCore.QRect(300, 360, 1280, 720))
        self.The_inference.setObjectName("The_inference")
        self.The_inference.setScaledContents(True) # 自适应大小
        self.The_inference.setFrameShape(QtWidgets.QFrame.Box)

        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(300, 600, 827, 22))
        self.menubar.setObjectName("menubar")
        MainWindow.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(MainWindow)
        self.statusbar.setObjectName("statusbar")
        MainWindow.setStatusBar(self.statusbar)

        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

        # 线程控制

    #阈值更新
    def updateNumber(self, text):
        try:
            number = float(text)
            print("数字改变！")
            print(number)
            self.threshold = number
        except ValueError:
            pass

    def From_files(self):
        # 获取当地选取图片的绝对url，并将其显示在The_image的label控件中
        the_image_url = QtWidgets.QFileDialog.getOpenFileName(None, 'select image', '', '')
        url = str(the_image_url).split("'")
        # ['(', 'C:/Users/Minghui_Zhang/Desktop/yolov5-opencv-cpp-python-main/target.jpg', ', ', 'All Files (*)', ')']
        print(url[1])
        # self.The_Url.setText(QtCore.QCoreApplication.translate("MainWindow", "路径：" + url[1]))

        self.target_path = url[1]
        # 判断文件是否是图片
        print(self.target_path)
        try:
            if not is_image_file(self.target_path):
                raise ValueError("Not a picture file")
        except ValueError:
            # 弹出提示框
            QtWidgets.QMessageBox.critical(self, "Error", "请选择一个合法的图片文件!")
            self.The_image.setText("请再导入图片")
            return
        img = QtGui.QPixmap(url[1])
        self.The_image.setPixmap(img)

    def Load_Videl(self):  # 加载视频的信号
        the_video_url = QtWidgets.QFileDialog.getOpenFileName(None, 'select video', '', '')
        video_url = str(the_video_url).split("'")
        # 调用yolo.py的方法加载cv2格式的视频,放入video_caputre属性
        self.video_path = video_url[1]

        # 判断文件是否是视频
        try:
            if not is_video_file(self.video_path):
                raise ValueError("Not a video file")
        except ValueError:
            # 弹出提示框
            QtWidgets.QMessageBox.critical(self, "Error", "请选择一个合法的视频文件!")
            self.The_video.setText("请再导入视频")
            return

        # 给出提示
        # QtWidgets.QMessageBox.information(self, "Success", "The selected file is a valid video file.")
        print("导入路径成功")
        self.The_video.setText("已导入视频")

    def Load_Model(self):
        self.model = ...
        self.threshold = 0.9998
        print(self.threshold)
        self.The_model.setText("已选择默认模型")


    def ReID_Inference(self):
        # self.inference = True

        # 创建 VideoThread 对象并启动线程
        self.video_thread = VideoThread(self)
        self.video_thread.new_frame_signal.connect(self.detect_and_show_frame)
        self.video_thread.start()

    def detect_and_show_frame(self, new_frame_signal):
        # print("传到这里来了！")
        self.The_inference.setPixmap(new_frame_signal)




    def Inference_Pause(self):
        self.video_thread.pause()

    def Inference_Wake(self):
        self.video_thread.resume()

    def Inference_Close(self):
        # 关闭 VideoThread 线程
        if self.video_thread is not None:
            # self.video_thread.pause()
            self.video_thread.close()
        print("结束推理")
        # event.accept()


    # def ReID_Inference(self):  # 推理
    #     self.inference = True
    #     print("开始！！！")
    #
    #     # yolov5的输入大小
    #     INPUT_WIDTH = 640
    #     INPUT_HEIGHT = 640
    #     time1 = time.time()
    #     colors = [(255, 255, 0), (0, 255, 0), (0, 255, 255), (255, 0, 0)]
    #     is_cuda = len(sys.argv) > 1 and sys.argv[1] == "cuda"
    #
    #     # 目标检测模型和reid模型
    #     root_path = os.path.dirname(os.getcwd())  # 项目根目录
    #
    #     print(root_path)
    #     yolov5_path = osp.join(root_path, 'config_files', 'yolov5s.onnx')
    #     reid_path = osp.join(root_path, 'config_files', 'mgn_.onnx')
    #     class_path = osp.join(root_path, 'config_files', 'classes.txt')
    #
    #     print("路径")
    #     print(yolov5_path)
    #     net = build_model(is_cuda, yolov5_path)
    #     reidModule = reid_model(is_cuda, reid_path)
    #     class_list = load_classes(class_path)
    #     threshold_similarity = self.threshold
    #     print("相似度阈值：")
    #     print(threshold_similarity)
    #     # 加载视频
    #     capture = load_capture(self.video_path)
    #     start = time.time_ns()
    #     frame_count = 0
    #     total_frames = 0
    #     fps = -1
    #
    #     # 视频规格
    #     width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
    #     height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
    #     print(width)
    #     print(height)
    #     fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    #     fpss = capture.get(cv2.CAP_PROP_FPS)
    #
    #     out = cv2.VideoWriter('output.mp4', fourcc, fpss, (width, height))
    #
    #     # 获取target的feature
    #     target = cv2.imread(self.target_path)
    #     target_feature = reidFeatrueExtract(target, reidModule)
    #     target_feature_norm = target_feature / np.linalg.norm(target_feature)
    #
    #     # 开始帧处理
    #     start_clock = time.time()
    #     print("开始帧处理")
    #     while self.inference:
    #
    #         # Frame 将获得视频中/相机中的下一帧的(通过“cap”)
    #         # Ret 将从相机中获取的帧中获得返回值，要么为true，要么为false
    #         _, frame = capture.read()
    #
    #         if frame is None:
    #             print("End of stream")
    #             break
    #
    #         #一半的概率不做检测，直接保存
    #         f_random = random.random()
    #         if f_random > 0.5 :
    #             frame_count += 1
    #             total_frames += 1
    #             # 保存
    #             out.write(frame)
    #             # cv2 BGR 转 PYQT5 RBG
    #             height, width, bytesPerComponent = frame.shape
    #             bytesPerLine = 3 * width
    #             cv2.cvtColor(frame, cv2.COLOR_BGR2RGB, frame)
    #             QImg = QImage(frame.data, width, height, bytesPerLine, QImage.Format_RGB888)
    #             pixmap = QPixmap.fromImage(QImg)
    #             self.The_inference.setPixmap(pixmap)
    #
    #             if cv2.waitKey(1) > -1:
    #                 print("finished by user1")
    #
    #             # if self.inference:
    #             #     while self.paused:
    #             #         ...
    #
    #             # print(self.inference)
    #             if not self.inference:
    #                 print("finished by user2")
    #                 break
    #             print("跳过")
    #             continue
    #
    #
    #         # 原帧的深拷贝
    #         # f_old = copy.deepcopy(frame)
    #         # 目标检测(行人检测)
    #         inputImage = format_yolov5(frame)
    #         outs = detect(inputImage, net, INPUT_WIDTH, INPUT_HEIGHT)
    #         class_ids, confidences, boxes = wrap_detection(inputImage, outs[0], INPUT_WIDTH, INPUT_HEIGHT)
    #         frame_count += 1
    #         total_frames += 1
    #         features = []
    #         coordinate = []
    #         # 绘制行人框，且提取所有行人特征，及其对应的帧坐标box
    #         for (classid, confidence, box) in zip(class_ids, confidences, boxes):
    #             # 如果是行人person的检测框，则提取该行人的特征
    #             if classid == 0:
    #                 color = colors[int(classid) % len(colors)]
    #                 a = np.array(frame[box[1]:box[1] + box[3], box[0]:box[0] + box[2]])
    #                 if a.shape[0] <= 0 or a.shape[1] <= 0:  # 非法box过滤img
    #                     continue
    #                 f = reidFeatrueExtract(a, reidModule)
    #                 f_norm = f / np.linalg.norm(f)
    #                 cos_sim = np.dot(f_norm, target_feature_norm)
    #                 # cos_sim = 0
    #                 # f = 0
    #                 # 对比识别
    #                 if cos_sim > threshold_similarity:
    #                     print(cos_sim)
    #                     # 输出人物出现时间
    #                     p_clock = time.time()
    #                     second = int(p_clock - start_clock)
    #                     minute = int(second / 60)
    #                     print("{}分{}秒".format(minute, second))
    #                     # 标记任务框框
    #                     cv2.rectangle(frame, box, color, 2)
    #                     cv2.rectangle(frame, (box[0], box[1] - 20), (box[0] + box[2], box[1]), color, -1)
    #                     cv2.putText(frame, class_list[classid], (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, .5,
    #                                 (0, 0, 0))
    #                 features.append(f)
    #                 # person对应坐标
    #                 coordinate.append(box)
    #             else:
    #                 continue
    #
    #         if frame_count >= 30:
    #             end = time.time()
    #             fps = 1000000000 * frame_count / (end - start)
    #             frame_count = 0
    #             start = time.time_ns()
    #
    #         if fps > 0:
    #             fps_label = "FPS: %.2f" % fps
    #             cv2.putText(frame, fps_label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
    #
    #         # 保存
    #         out.write(frame)
    #         # cv2 BGR 转 PYQT5 RBG
    #         height, width, bytesPerComponent = frame.shape
    #         bytesPerLine = 3 * width
    #         cv2.cvtColor(frame, cv2.COLOR_BGR2RGB, frame)
    #         QImg = QImage(frame.data, width, height, bytesPerLine, QImage.Format_RGB888)
    #         pixmap = QPixmap.fromImage(QImg)
    #         self.The_inference.setPixmap(pixmap)
    #
    #         if cv2.waitKey(1) > -1:
    #             print("finished by user1")
    #
    #         # if self.inference:
    #         #     while self.paused:
    #         #         ...
    #
    #         # print(self.inference)
    #         if not self.inference:
    #             print("finished by user2")
    #             break
    #
    #     print("Total frames: " + str(total_frames))
    #     time2 = time.time()
    #     print("total time: " + str(time2 - time1))
    #     capture.release()
    #     out.release()
    #     cv2.destroyAllWindows()



    def retranslateUi(self, MainWindow):
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setWindowTitle(_translate("MainWindow", "行人重识别监测系统"))
        self.Image.setText(_translate("MainWindow", "导入图片"))
        self.Video.setText(_translate("MainWindow", "导入视频"))
        self.Model.setText(_translate("MainWindow", "选择模型"))
        self.Inference.setText(_translate("MainWindow", "ReID推理"))
        self.The_image.setText(_translate("MainWindow", "请导入图片"))
        self.The_video.setText(_translate("MainWindow", "请导入视频"))
        self.The_model.setText(_translate("MainWindow", "请选择模型"))
        self.The_threshold.setText(_translate("MainWindow", "请输入相似度阈值"))
        self.The_inference.setText(_translate("MainWindow", "请推理"))
        self.Stop.setText(_translate("MainWindow", "暂停推理"))
        self.Wake.setText(_translate("MainWindow", "继续推理"))
        self.Close.setText(_translate("MainWindow", "关闭推理"))



if __name__ == '__main__':
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())